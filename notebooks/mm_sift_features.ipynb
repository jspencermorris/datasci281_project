{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from utils import *\n",
    "\n",
    "# Important NOTE: Use opencv >=4.4 \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global tunable knobs\n",
    "\n",
    "# Debug prints\n",
    "debug = True\n",
    "\n",
    "# Number of K-Means clusters needed for BoVW \n",
    "NUM_KMEANS_CLUSTER = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/validation/test subsets were loaded from a pre-generated file\n",
      "\tNumber of train files: 4799\n",
      "\tNumber of val files: 1599\n",
      "\tNumber of test files: 1601\n"
     ]
    }
   ],
   "source": [
    "# define file directory\n",
    "directory = '../data/interim/PatternNet/PatternNet/images'\n",
    "\n",
    "# create a list of classes considered for this project\n",
    "classes = ['beach', 'chaparral', 'dense_residential', 'forest', 'freeway', 'harbor', 'overpass', 'parking_space', 'river', 'swimming_pool']\n",
    "\n",
    "# define the train, val, and test sets\n",
    "train_files, val_files, test_files = generate_splits(classes, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_points(img):\n",
    "\n",
    "    # Converting image to grayscale\n",
    "    gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Applying SIFT detector\n",
    "    sift = cv2.SIFT_create(nfeatures=0, nOctaveLayers=3, edgeThreshold=0.2, contrastThreshold=0.07)\n",
    "\n",
    "    #kp = sift.detect(gray, None)\n",
    "    kp, des = sift.detectAndCompute(gray,None)\n",
    "\n",
    "    # Marking the keypoint on the image using circles\n",
    "    sift_img=cv2.drawKeypoints(gray, kp, img,\n",
    "                          flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "    return sift_img, des\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to show a grid of SIFT image in a directory (given a file subset)\n",
    "def visualize_sift(files, directory, images_per_class=3):\n",
    "    # create a dictionary to store class images\n",
    "    class_images = {}\n",
    "    \n",
    "    # iterate over each file\n",
    "    for class_name, file_name in files:\n",
    "        # load the image\n",
    "        img = plt.imread(os.path.join(directory, class_name, file_name))\n",
    "        # if class not in dictionary, initialize empty list\n",
    "        if class_name not in class_images:\n",
    "            class_images[class_name] = []\n",
    "        # append image to class list\n",
    "        class_images[class_name].append(img)\n",
    "\n",
    "    # create a grid of images\n",
    "    num_classes = len(class_images)\n",
    "    fig, axes = plt.subplots(num_classes, images_per_class + 1, figsize=(12, 3*num_classes))\n",
    "    for i, (class_name, images) in enumerate(class_images.items()):\n",
    "        # display class name in the first column\n",
    "        axes[i, 0].text(0.5, 0.5, class_name, fontsize=16, ha='center', va='center')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # display random images in the subsequent columns\n",
    "        random.shuffle(images)\n",
    "        for j in range(images_per_class):\n",
    "            sift_image = extract_key_points(images[j])\n",
    "            axes[i, j+1].imshow(sift_image)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get keypoint descriptors for a subset of images (per class) in a directory \n",
    "def generate_sift_vectors (files, directory, images_per_class=3):\n",
    "    # create a dictionary to store class images\n",
    "    class_images = {}\n",
    "    \n",
    "    # create an empty array to store labels and keypoint descriptors of all images\n",
    "    desc_list = []\n",
    "    label_list = []\n",
    "\n",
    "    # iterate over each file\n",
    "    for class_name, file_name in files:\n",
    "        # load the image\n",
    "        img = plt.imread(os.path.join(directory, class_name, file_name))\n",
    "        # if class not in dictionary, initialize empty list\n",
    "        if class_name not in class_images:\n",
    "            class_images[class_name] = []\n",
    "        # append image to class list\n",
    "        class_images[class_name].append(img)\n",
    "\n",
    "    # For each subset of images in a class, extract keypoints\n",
    "    num_classes = len(class_images)\n",
    "    for i, (class_name, images) in enumerate(class_images.items()):\n",
    "        random.shuffle(images)\n",
    "        for j in range(images_per_class):\n",
    "            sift_image, desc = extract_key_points(images[j])\n",
    "            desc_list.append(desc)\n",
    "            label_list.append(class_name)\n",
    "\n",
    "    return desc_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract SIFT keypoints from all training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: Labels: 4000 KP_Descriptors: 4000\n",
      "(1419665, 128)\n"
     ]
    }
   ],
   "source": [
    "# generate the set of feature vectors for all images in each class of the training set\n",
    "train_sift_desc_list, train_label_list = generate_sift_vectors(train_files, directory, images_per_class=400)\n",
    "\n",
    "\n",
    "# sift_desc_list is a 2D array per image: (Number of keypoints X 128).\n",
    "# the Number of keypoints per image can vary from 0 to several thousands.\n",
    "# label_list is a 1D array containing the class label for all those images.\n",
    "\n",
    "if debug:\n",
    "    print(f'Number of train images: Labels: {len(train_label_list)} KP_Descriptors: {len(train_sift_desc_list)}')\n",
    "\n",
    "# Stack all the kp descriptors vertically so that we get one giant 2D array\n",
    "# Number of keypoints across all images X 128\n",
    "vStack = np.array(train_sift_desc_list[0])\n",
    "for remaining in train_sift_desc_list[1:]:\n",
    "    if remaining is not None:\n",
    "        vStack = np.vstack((vStack, remaining))\n",
    "\n",
    "if debug:\n",
    "    print(vStack.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the bag of Visual words\n",
    "Feed all KP descriptors into KMeans algorithm attempting to create a max of NUM_KMEANS_CLUSTERS based on the keypoints extracted from all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of KPs: (1419665,), MinCluster: 0, MaxCluster: 19\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(init=\"k-means++\", n_clusters=NUM_KMEANS_CLUSTER, n_init=4)\n",
    "kmeans_fit = kmeans.fit_predict(vStack)\n",
    "\n",
    "if debug:\n",
    "    print(f'Number of KPs: {kmeans_fit.shape}, MinCluster: {kmeans_fit.min()}, MaxCluster: {kmeans_fit.max()}')\n",
    "\n",
    "# Create a histogram for each image - each kp for the image belongs to one bucket (visual word)\n",
    "# Keep a count of such visual words per image.\n",
    "num_images = len(train_sift_desc_list)\n",
    "histogram = np.array([np.zeros(NUM_KMEANS_CLUSTER) for i in range(num_images)])\n",
    "old_count = 0\n",
    "for img_num in range(num_images):\n",
    "    if train_sift_desc_list[img_num] is None:\n",
    "        # Some images have zero keypoints. Skip\n",
    "        continue\n",
    "    num_kp_in_image = len(train_sift_desc_list[img_num])\n",
    "    for j in range(num_kp_in_image):\n",
    "        idx = kmeans_fit[old_count+j]\n",
    "        histogram[img_num][idx] += 1\n",
    "    old_count += num_kp_in_image\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector for supervised training\n",
    "Since the histogram for each image carries a (weighted) combination of visual words, feed this data into SVM with the labels from the training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = histogram\n",
    "y_train = train_label_list\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "svm_clf = SVC(random_state=42)\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "#svc = SVC().fit(histogram, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Extract the SIFT keypoints and create the BoVW histogram.\n",
    "Feed this histogram into the classifier to predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of val images: Labels: 400 KP_Descriptors: 400\n"
     ]
    }
   ],
   "source": [
    "# generate the set of feature vectors for all images in each class of the training set\n",
    "val_sift_desc_list, val_label_list = generate_sift_vectors(val_files, directory, images_per_class=40)\n",
    "if debug:\n",
    "    print(f'Number of val images: Labels: {len(val_label_list)} KP_Descriptors: {len(val_sift_desc_list)}')\n",
    "    \n",
    "num_images = len(val_sift_desc_list)\n",
    "words = np.array([np.zeros(NUM_KMEANS_CLUSTER) for i in range(num_images)])\n",
    "old_count = 0\n",
    "for i in range(num_images):\n",
    "    if val_sift_desc_list[i] is None:\n",
    "        continue\n",
    "    l = len(val_sift_desc_list[i])\n",
    "    ret = kmeans.predict(val_sift_desc_list[i])\n",
    "    for j in ret:\n",
    "        words[i][j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overpass, freeway\n",
      "overpass, freeway\n",
      "overpass, freeway\n",
      "overpass, freeway\n",
      "overpass, swimming_pool\n",
      "overpass, forest\n",
      "overpass, freeway\n",
      "overpass, freeway\n",
      "overpass, beach\n",
      "overpass, beach\n",
      "beach, freeway\n",
      "beach, forest\n",
      "beach, parking_space\n",
      "beach, dense_residential\n",
      "beach, overpass\n",
      "beach, river\n",
      "river, freeway\n",
      "river, freeway\n",
      "forest, river\n",
      "forest, river\n",
      "forest, river\n",
      "forest, river\n",
      "forest, river\n",
      "forest, river\n",
      "forest, river\n",
      "swimming_pool, dense_residential\n",
      "swimming_pool, dense_residential\n",
      "swimming_pool, dense_residential\n",
      "swimming_pool, dense_residential\n",
      "swimming_pool, dense_residential\n",
      "swimming_pool, harbor\n",
      "swimming_pool, harbor\n",
      "swimming_pool, beach\n",
      "swimming_pool, beach\n",
      "swimming_pool, dense_residential\n",
      "swimming_pool, dense_residential\n",
      "harbor, swimming_pool\n",
      "harbor, dense_residential\n",
      "harbor, swimming_pool\n",
      "parking_space, forest\n",
      "parking_space, beach\n",
      "parking_space, beach\n",
      "parking_space, freeway\n",
      "parking_space, beach\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, overpass\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, overpass\n",
      "freeway, beach\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, parking_space\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, river\n",
      "freeway, river\n",
      "dense_residential, swimming_pool\n",
      "dense_residential, swimming_pool\n",
      "dense_residential, harbor\n",
      "dense_residential, swimming_pool\n",
      "Matches: 332; Mismatches: 68\n"
     ]
    }
   ],
   "source": [
    "#scalar = StandardScaler().fit(vocab)\n",
    "#vocab = scalar.transform(vocab)\n",
    "\n",
    "X_val = words\n",
    "y_val = val_label_list\n",
    "\n",
    "norm_X_val = scaler.transform(X_val)\n",
    "y_pred = svm_clf.predict(norm_X_val)\n",
    "\n",
    "match_count = 0\n",
    "for i in range(len(y_val)):\n",
    "    if y_val[i] != y_pred[i]:\n",
    "        print(f'{y_val[i]}, {y_pred[i]}')\n",
    "    else:\n",
    "        match_count += 1\n",
    "        \n",
    "print(f'Matches: {match_count}; Mismatches: {len(y_val)-match_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/interim/PatternNet/images/parking_space/parkingspace005.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     13\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/interim/PatternNet/images\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparking_space\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparkingspace005.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 15\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Converting image to grayscale\u001b[39;00m\n\u001b[1;32m     18\u001b[0m gray\u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img,cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/matplotlib/pyplot.py:2168\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2166\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimread)\n\u001b[1;32m   2167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(fname, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 2168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/matplotlib/image.py:1561\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parse\u001b[38;5;241m.\u001b[39murlparse(fname)\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;66;03m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease open the URL for reading and pass the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult to Pillow, e.g. with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1560\u001b[0m         )\n\u001b[0;32m-> 1561\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimg_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[1;32m   1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1563\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m             pil_to_array(image))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3227\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/interim/PatternNet/images/parking_space/parkingspace005.jpg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "\n",
    "# Important NOTE: Use opencv >=4.4 \n",
    "import cv2\n",
    "\n",
    "sample = '../data/interim/PatternNet/images' + '/' + 'parking_space' + '/' + 'parkingspace005.jpg'\n",
    "\n",
    "img = plt.imread(sample)\n",
    "\n",
    "# Converting image to grayscale\n",
    "gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Applying SIFT detector\n",
    "sift = cv2.SIFT_create(nfeatures=0, nOctaveLayers=3, edgeThreshold=0.2, contrastThreshold=0.07)\n",
    "\n",
    "#kp = sift.detect(gray, None)\n",
    "kp, des = sift.detectAndCompute(gray,None)\n",
    "\n",
    "# Marking the keypoint on the image using circles\n",
    "sift_img=cv2.drawKeypoints(gray, kp, img,\n",
    "                          flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.imshow(sift_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kushalvyas.github.io/BOV.html\n",
    "\n",
    "https://www.vlfeat.org/api/sift.html#sift-tech-descriptor\n",
    "\n",
    "https://docs.opencv.org/3.4/d7/d60/classcv_1_1SIFT.html\n",
    "\n",
    "https://machinelearningmastery.com/opencv_sift_surf_orb_keypoints\n",
    "\n",
    "https://github.com/kushalvyas/Bag-of-Visual-Words-Python/blob/8ddda6ab804f14777855c8f4119f749f61e2da6e/Bag.py#L140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
