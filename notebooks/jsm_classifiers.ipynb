{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to save the feature vector dictionary to disk\n",
    "def load_feature_data(vectors_filepath, names_filepath):\n",
    "    \n",
    "    # extract the JSON file from the tar.gz file\n",
    "    with tarfile.open(vectors_filepath, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "        json_filename = tar.getnames()[0]  # assuming only one file in the archive\n",
    "    \n",
    "    # load the JSON file and convert Python lists back to numpy arrays\n",
    "    with open(json_filename, 'r') as f:\n",
    "        feature_vectors_dict = json.load(f)\n",
    "    \n",
    "    feature_vectors = {}\n",
    "    for class_name, vectors in feature_vectors_dict.items():\n",
    "        feature_vectors[class_name] = [np.array(vector) for vector in vectors]\n",
    "    \n",
    "    # remove the extracted JSON file\n",
    "    os.remove(json_filename)\n",
    "    \n",
    "    # load feature names\n",
    "    with open(names_filepath, 'rb') as f:\n",
    "        feature_names = pickle.load(f)\n",
    "        \n",
    "    return feature_vectors, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to apply transformations prior to modeling\n",
    "def preprocess_data(feature_vectors, scaler, pca_model):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for class_name, vectors in feature_vectors.items():\n",
    "        for vector in vectors:\n",
    "            X.append(vector)\n",
    "            y.append(class_name)\n",
    "    \n",
    "    # z-standardization\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # PCA transformation\n",
    "    X_pca = pca_model.transform(X_scaled)\n",
    "    \n",
    "    return X_pca, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to train data on RFC and SVM using grid search\n",
    "def train_models(X_train, y_train):\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # Random Forest Classifier\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "    # specify model parameters to analyze\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20]\n",
    "    }\n",
    "    rf_grid_search = GridSearchCV(rf_classifier, rf_params, cv=5, scoring='accuracy')\n",
    "    rf_grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(\"Best parameters for Random Forest Classifier:\", rf_grid_search.best_params_)\n",
    "    print(\"Training time for Random Forest Classifier: elapsed_time\")\n",
    "    \n",
    "\n",
    "    # Support Vector Machine\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    svc_classifier = SVC(random_state=42)\n",
    "    # specify model parameters to analyze\n",
    "    svc_params = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "    svc_grid_search = GridSearchCV(svc_classifier, svc_params, cv=5, scoring='accuracy')\n",
    "    svc_grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(\"Best parameters for Support Vector Machine:\", svc_grid_search.best_params_)\n",
    "    print(\"Training time for Support Vector Machine: elapsed_time\")\n",
    "    \n",
    "    return rf_grid_search, svc_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to evaluate the trained model on the training and validation sets\n",
    "def evaluate_models(model, X_train, y_train, X_val, y_val):\n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_val)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, train_preds)\n",
    "    val_accuracy = accuracy_score(y_val, val_preds)\n",
    "    \n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Validation Accuracy:\", val_accuracy)\n",
    "    \n",
    "    return train_preds, val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to visualize the confusion matrices\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title):\n",
    "    # compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    \n",
    "    # plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    # plot the confusion matrix as an image with blue colors\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    # define the tick marks for the classes\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    # add text annotations for each cell\n",
    "    fmt = 'd'\n",
    "    # set a 50% threshold to improve text coloration\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file directory\n",
    "directory = '../data/interim/PatternNet/PatternNet/images'\n",
    "\n",
    "# create a list of classes considered for this project\n",
    "classes = ['beach', 'chaparral', 'dense_residential', 'forest', 'freeway', 'harbor', 'overpass', 'parking_space', 'river', 'swimming_pool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature data from disk\n",
    "feature_vectors_train, feature_names = load_feature_data(\"../data/processed/feature_vectors_train.tar.gz\", \"../data/processed/feature_names.pkl\")\n",
    "feature_vectors_val, feature_names = load_feature_data(\"../data/processed/feature_vectors_val.tar.gz\", \"../data/processed/feature_names.pkl\")\n",
    "feature_vectors_test, feature_names = load_feature_data(\"../data/processed/feature_vectors_test.tar.gz\", \"../data/processed/feature_names.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load z-standardization scaler\n",
    "with open(\"../data/processed/features_scaler.pkl\", 'rb') as f:\n",
    "    features_scaler = pickle.load(f)\n",
    "\n",
    "# load pca model\n",
    "with open(\"../data/processed/pca_model.pkl\", 'rb') as f:\n",
    "    pca_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "X_train, y_train = preprocess_data(feature_vectors_train, features_scaler, pca_model)\n",
    "X_val, y_val = preprocess_data(feature_vectors_val, features_scaler, pca_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "rf_model, svc_model = train_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate models\n",
    "\n",
    "print(\"Random Forest Classifier:\")\n",
    "train_preds_rf, val_preds_rf = evaluate_models(rf_model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\nSupport Vector Machine:\")\n",
    "train_preds_svc, val_preds_svc = evaluate_models(svc_model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot confusion matrices\n",
    "\n",
    "plot_confusion_matrix(y_train, train_preds_rf, classes, title='Random Forest Classifier - Train')\n",
    "plot_confusion_matrix(y_val, val_preds_rf, classes, title='Random Forest Classifier - Validation')\n",
    "\n",
    "plot_confusion_matrix(y_train, train_preds_svc, classes, title='Support Vector Machine - Train')\n",
    "plot_confusion_matrix(y_val, val_preds_svc, classes, title='Support Vector Machine - Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect mismatches\n",
    "Loading the list of all training/validation/test images from a precooked json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the JSON file from the tar.gz file\n",
    "with tarfile.open('../data/processed/image_list_test.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "    json_filename = tar.getnames()[0]  # assuming only one file in the archive\n",
    "    \n",
    "    # load the JSON file and convert Python lists back to numpy arrays\n",
    "    with open(json_filename, 'r') as f:\n",
    "        images_dict = json.load(f)    \n",
    "\n",
    "display_counts_per_class(json_filename, images_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_v = \"\"\n",
    "\n",
    "# Iterate through all Y of the validation dataset\n",
    "for k, v in enumerate(y_val):\n",
    "    if old_v != v:\n",
    "        offset_in_class = 0\n",
    "        old_v = v\n",
    "    else:\n",
    "        offset_in_class += 1\n",
    "    \n",
    "    # Whenever predicted class varies from the true class\n",
    "    if y_val[k] != val_preds_svc[k]:\n",
    "        #print(k, v, old_v, offset_in_class)\n",
    "        print(f'True Class: {y_val[k]}         Predicted Class: {val_preds_svc[k]}')\n",
    "        img_name = images_dict[v][offset_in_class]\n",
    "        #print(img_name)\n",
    "        \n",
    "        # Load the JPEG image\n",
    "        img = mpimg.imread(img_name)\n",
    "\n",
    "        # Display the image\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Turn off axis\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique values and their counts\n",
    "unique_values, counts = np.unique(y_val, return_counts=True)\n",
    "\n",
    "# Print unique values and their counts\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"{value}: {count}\")\n",
    "\n",
    "np.sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique values and their counts\n",
    "unique_values, counts = np.unique(val_preds_svc, return_counts=True)\n",
    "\n",
    "# Print unique values and their counts\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"{value}: {count}\")\n",
    "\n",
    "np.sum(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
