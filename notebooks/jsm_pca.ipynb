{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to save the feature vector dictionary to disk\n",
    "def load_feature_data(vectors_filepath, names_filepath):\n",
    "    \n",
    "    # extract the JSON file from the tar.gz file\n",
    "    with tarfile.open(vectors_filepath, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "        json_filename = tar.getnames()[0]  # assuming only one file in the archive\n",
    "    \n",
    "    # load the JSON file and convert Python lists back to numpy arrays\n",
    "    with open(json_filename, 'r') as f:\n",
    "        feature_vectors_dict = json.load(f)\n",
    "    \n",
    "    feature_vectors = {}\n",
    "    for class_name, vectors in feature_vectors_dict.items():\n",
    "        feature_vectors[class_name] = [np.array(vector) for vector in vectors]\n",
    "    \n",
    "    # remove the extracted JSON file\n",
    "    os.remove(json_filename)\n",
    "    \n",
    "    # load feature names\n",
    "    with open(names_filepath, 'rb') as f:\n",
    "        feature_names = pickle.load(f)\n",
    "        \n",
    "    return feature_vectors, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to visualize the PCA elbow plot\n",
    "def evaluate_pca(pca):\n",
    "    \n",
    "    # compute the cumulative explained variance\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # plot the elbow plot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-')\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative % of Variation Explained')\n",
    "    plt.title('Principal Component Elbow Plot')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # find the location of inflection point\n",
    "    # NOTE: this is used to determine the best number of PC's\n",
    "    diff = np.diff(cumulative_variance)\n",
    "    inflection_point = np.argmax(diff < np.mean(diff))\n",
    "    plt.axvline(x=inflection_point + 1, color='r', linestyle='--')\n",
    "    plt.axhline(y=cumulative_variance[inflection_point], color='r', linestyle='--')\n",
    "    \n",
    "    # add label to indicate inflection point\n",
    "    plt.text(inflection_point + 50, cumulative_variance[inflection_point] - .2,\n",
    "            f\"n_components={inflection_point + 1}\\ncum_%_variation={cumulative_variance[inflection_point]*100:.2f}%\", color='r', fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return inflection_point + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to display the main features that contribute to a set of top PC's\n",
    "def display_loadings(pca, n_pc=5, n_features=10):\n",
    "    \n",
    "    # extract top principal components\n",
    "    top_components = pca.components_[:n_pc]\n",
    "\n",
    "    # create dataframes for top principal components\n",
    "    for i, component in enumerate(top_components):\n",
    "        loading_scores = pd.DataFrame({'feature_name': feature_names, 'loading_score': component})\n",
    "        loading_scores = loading_scores.sort_values(by='loading_score', ascending=False).head(n_features)\n",
    "        print(f\"Top {n_features} features for Principal Component {i + 1}:\")\n",
    "        display(loading_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file directory\n",
    "directory = '../data/interim/PatternNet/PatternNet/images'\n",
    "\n",
    "# create a list of classes considered for this project\n",
    "classes = ['beach', 'chaparral', 'dense_residential', 'forest', 'freeway', 'harbor', 'overpass', 'parking_space', 'river', 'swimming_pool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature data from disk\n",
    "feature_vectors, feature_names = load_feature_data(\"../data/processed/feature_vectors_2.tar.gz\", \"../data/processed/feature_names_2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspections\n",
    "print(type(feature_vectors))\n",
    "print(feature_vectors.keys())\n",
    "print(type(feature_vectors['beach']))\n",
    "print(len(feature_vectors['beach']))\n",
    "print(type(feature_vectors['beach'][0]))\n",
    "print(len(feature_vectors['beach'][0]))\n",
    "print(feature_vectors['beach'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten feature vectors and prepare data for PCA\n",
    "X = []\n",
    "for class_name in classes:\n",
    "    X.extend(feature_vectors[class_name])\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "\n",
    "# standardize data\n",
    "features_scaler = StandardScaler()\n",
    "X = features_scaler.fit_transform(X)\n",
    "\n",
    "# save the standardization scaler to disk\n",
    "with open(\"../data/processed/features_scaler.pkl\", 'wb') as f:\n",
    "    pickle.dump(features_scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA\n",
    "pca_model = PCA()\n",
    "pca_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_components = evaluate_pca(pca_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loadings(pca_model, n_pc=5, n_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-compute pca model with optimum number of PC's\n",
    "pca_model = PCA(n_components=n_pc_components)\n",
    "pca_model.fit(X)\n",
    "\n",
    "# save the pca model to disk\n",
    "with open(\"../data/processed/pca_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(pca_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
