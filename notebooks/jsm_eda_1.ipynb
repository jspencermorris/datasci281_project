{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate the ML subsets\n",
    "def generate_splits(classes, directory, split_file=\"../data/processed/split_definition.json\"):\n",
    "    # initialize empty lists to store train, validation, and test filenames\n",
    "    train_files, val_files, test_files = [], [], []\n",
    "    \n",
    "    # check if split definition file exists\n",
    "    if os.path.exists(split_file):\n",
    "        print(\"train/validation/test subsets were loaded from a pre-generated file\")\n",
    "        # load split definition file\n",
    "        with open(split_file, 'r') as file:\n",
    "            split_data = json.load(file)\n",
    "            train_files = split_data['Train']\n",
    "            val_files = split_data['Validation']\n",
    "            test_files = split_data['Test']\n",
    "            \n",
    "    else:\n",
    "        print(\"train/validation/test subsets were generated and saved to a file\")\n",
    "        # iterate over each class\n",
    "        for class_name in classes:\n",
    "            # get the directory path for the current class\n",
    "            class_dir = os.path.join(directory, class_name)\n",
    "            # list all files in the directory\n",
    "            files = os.listdir(class_dir)\n",
    "            # shuffle the list of files\n",
    "            np.random.shuffle(files)\n",
    "            # calculate split points\n",
    "            total_files = len(files)\n",
    "            train_split = int(total_files * 0.6)\n",
    "            val_split = int(total_files * 0.2)\n",
    "            # assign files to train, validation, and test sets\n",
    "            train_files.extend([(class_name, file) for file in files[:train_split]])\n",
    "            val_files.extend([(class_name, file) for file in files[train_split:train_split+val_split]])\n",
    "            test_files.extend([(class_name, file) for file in files[train_split+val_split:]])\n",
    "            \n",
    "        # shuffle the train, validation, and test sets\n",
    "        np.random.shuffle(train_files)\n",
    "        np.random.shuffle(val_files)\n",
    "        np.random.shuffle(test_files)\n",
    "        \n",
    "        # save split definition to a json file\n",
    "        with open(split_file, 'w') as file:\n",
    "            json.dump({'Train': train_files, 'Validation': val_files, 'Test': test_files}, file)\n",
    "            \n",
    "    # display the number of files in each set\n",
    "    print(\"\\tNumber of train files:\", len(train_files))\n",
    "    print(\"\\tNumber of val files:\", len(val_files))\n",
    "    print(\"\\tNumber of test files:\", len(test_files))\n",
    "    \n",
    "    return train_files, val_files, test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that determines the mean and standard deviation of each RGB and and L*a*b*\n",
    "# color-space channel for an image\n",
    "def compute_channel_stats(image_path):\n",
    "    # read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # convert image to L*a*b* color space\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    \n",
    "    # compute mean and standard deviation for each color channel (RGB and L*a*b*)\n",
    "    mean_rgb, std_rgb = cv2.meanStdDev(img)\n",
    "    mean_lab, std_lab = cv2.meanStdDev(lab_img)\n",
    "    \n",
    "    # flatten the results into a feature vector\n",
    "    channel_stats = np.concatenate((mean_rgb.flatten(), std_rgb.flatten(), mean_lab.flatten(), std_lab.flatten()))\n",
    "    \n",
    "    return channel_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that loops through each file to generate a dictionary that contains\n",
    "# the feature vectors of all images in each class\n",
    "def generate_feature_vectors(files, directory):\n",
    "    feature_vectors = {}\n",
    "    \n",
    "    # iterate over each file\n",
    "    for class_name, file_name in files:\n",
    "        # load the image\n",
    "        img_path = os.path.join(directory, class_name, file_name)\n",
    "        \n",
    "        # compute color statistics\n",
    "        stats = compute_channel_stats(img_path)\n",
    "        # append each channel_stats array to the correct class in feature_vectors\n",
    "        if class_name not in feature_vectors:\n",
    "            feature_vectors[class_name] = []\n",
    "        feature_vectors[class_name].append(stats)\n",
    "        \n",
    "        # TODO:  add additional features to the feature_vector (HOG, SIFT, others?)\n",
    "    \n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/validation/test subsets were loaded from a pre-generated file\n",
      "\tNumber of train files: 4799\n",
      "\tNumber of val files: 1599\n",
      "\tNumber of test files: 1601\n"
     ]
    }
   ],
   "source": [
    "# define file directory\n",
    "directory = '../data/interim/PatternNet/PatternNet/images'\n",
    "\n",
    "# create a list of classes considered for this project\n",
    "classes = ['beach', 'chaparral', 'dense_residential', 'forest', 'freeway', 'harbor', 'overpass', 'parking_space', 'river', 'swimming_pool']\n",
    "\n",
    "# define the train, val, and test sets\n",
    "train_files, val_files, test_files = generate_splits(classes, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the set of feature vectors for all images in each class\n",
    "feature_vectors = generate_feature_vectors(train_files, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['parking_space', 'beach', 'forest', 'overpass', 'river', 'dense_residential', 'swimming_pool', 'chaparral', 'freeway', 'harbor'])\n",
      "<class 'list'>\n",
      "479\n",
      "[113.95576477 127.46353149 119.84751892  22.51988622  21.80362058\n",
      "  22.92205582 133.3782196  122.48179626 134.45556641  21.85814155\n",
      "   1.91485269   1.31443199]\n"
     ]
    }
   ],
   "source": [
    "# inspections\n",
    "\n",
    "print(type(feature_vectors))\n",
    "print(feature_vectors.keys())\n",
    "print(type(feature_vectors['beach']))\n",
    "print(len(feature_vectors['beach']))\n",
    "print(feature_vectors['beach'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_281a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
