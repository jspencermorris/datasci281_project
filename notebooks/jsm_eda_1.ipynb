{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate the ML subsets\n",
    "def generate_splits(classes, directory, split_file=\"../data/processed/split_definition.json\"):\n",
    "    # initialize empty lists to store train, validation, and test filenames\n",
    "    train_files, val_files, test_files = [], [], []\n",
    "    \n",
    "    # check if split definition file exists\n",
    "    if os.path.exists(split_file):\n",
    "        print(\"train/validation/test subsets were loaded from a pre-generated file\")\n",
    "        # load split definition file\n",
    "        with open(split_file, 'r') as file:\n",
    "            split_data = json.load(file)\n",
    "            train_files = split_data['Train']\n",
    "            val_files = split_data['Validation']\n",
    "            test_files = split_data['Test']\n",
    "            \n",
    "    else:\n",
    "        print(\"train/validation/test subsets were generated and saved to a file\")\n",
    "        # iterate over each class\n",
    "        for class_name in classes:\n",
    "            # get the directory path for the current class\n",
    "            class_dir = os.path.join(directory, class_name)\n",
    "            # list all files in the directory\n",
    "            files = os.listdir(class_dir)\n",
    "            # shuffle the list of files\n",
    "            np.random.shuffle(files)\n",
    "            # calculate split points\n",
    "            total_files = len(files)\n",
    "            train_split = int(total_files * 0.6)\n",
    "            val_split = int(total_files * 0.2)\n",
    "            # assign files to train, validation, and test sets\n",
    "            train_files.extend([(class_name, file) for file in files[:train_split]])\n",
    "            val_files.extend([(class_name, file) for file in files[train_split:train_split+val_split]])\n",
    "            test_files.extend([(class_name, file) for file in files[train_split+val_split:]])\n",
    "            \n",
    "        # shuffle the train, validation, and test sets\n",
    "        np.random.shuffle(train_files)\n",
    "        np.random.shuffle(val_files)\n",
    "        np.random.shuffle(test_files)\n",
    "        \n",
    "        # save split definition to a json file\n",
    "        with open(split_file, 'w') as file:\n",
    "            json.dump({'Train': train_files, 'Validation': val_files, 'Test': test_files}, file)\n",
    "            \n",
    "    # display the number of files in each set\n",
    "    print(\"\\tNumber of train files:\", len(train_files))\n",
    "    print(\"\\tNumber of val files:\", len(val_files))\n",
    "    print(\"\\tNumber of test files:\", len(test_files))\n",
    "    \n",
    "    return train_files, val_files, test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that determines the mean and standard deviation of each RGB and and L*a*b*\n",
    "# color-space channel for an image\n",
    "def compute_channel_stats(image_path):\n",
    "    # read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # convert image to L*a*b* color space\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    \n",
    "    # compute mean and standard deviation for each color channel (RGB and L*a*b*)\n",
    "    mean_rgb, std_rgb = cv2.meanStdDev(img)\n",
    "    mean_lab, std_lab = cv2.meanStdDev(lab_img)\n",
    "    \n",
    "    # flatten the results into a feature vector\n",
    "    channel_stats = np.concatenate((mean_rgb.flatten(), std_rgb.flatten(), mean_lab.flatten(), std_lab.flatten()))\n",
    "    \n",
    "    return channel_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that determines the hog descriptors for an image's grayscale representation\n",
    "def compute_hog_stats(image_path):\n",
    "    # read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # convert image to grayscale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # compute HOG features\n",
    "    fd, hog_image = hog(gray_img, orientations=4, pixels_per_cell=(32, 32), visualize=True)\n",
    "    \n",
    "    return fd, hog_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that loops through each file to generate a dictionary that contains\n",
    "# the feature vectors of all images in each class\n",
    "def generate_feature_vectors(files, directory):\n",
    "    feature_vectors = {}\n",
    "    hog_images = {}\n",
    "    \n",
    "    # iterate over each file\n",
    "    for class_name, file_name in files:\n",
    "        # load the image\n",
    "        img_path = os.path.join(directory, class_name, file_name)\n",
    "        \n",
    "        # compute color statistics\n",
    "        channel_stats = compute_channel_stats(img_path)\n",
    "        \n",
    "        # compute HOG features\n",
    "        hog_stats, hog_image = compute_hog_stats(img_path)\n",
    "        \n",
    "        # concatenate color statistics and HOG features\n",
    "        stats = np.concatenate((channel_stats, hog_stats))\n",
    "        \n",
    "        # append feature vector to the correct class in feature_vectors\n",
    "        if class_name not in feature_vectors:\n",
    "            feature_vectors[class_name] = []\n",
    "        feature_vectors[class_name].append(stats)\n",
    "        \n",
    "        # append hog_image to the correct class in hog_images\n",
    "        if class_name not in hog_images:\n",
    "            hog_images[class_name] = []\n",
    "        hog_images[class_name].append(hog_image)\n",
    "        \n",
    "        \n",
    "        # TODO:  add additional features to the feature_vector (SIFT, spatial frequencies, texture, others?)\n",
    "    \n",
    "    return feature_vectors, hog_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to save the feature vector dictionary to disk\n",
    "def save_feature_vectors(feature_vectors, filename):\n",
    "    # convert numpy arrays to Python lists\n",
    "    feature_vectors_dict = {}\n",
    "    for class_name, vectors in feature_vectors.items():\n",
    "        feature_vectors_dict[class_name] = [vector.tolist() for vector in vectors]\n",
    "    \n",
    "    # save feature_vectors_dict dictionary as JSON\n",
    "    json_filename = filename.replace('.tar.gz', '.json')\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(feature_vectors_dict, f)\n",
    "    \n",
    "    # create tar.gz file\n",
    "    with tarfile.open(filename, 'w:gz') as tar:\n",
    "        tar.add(json_filename, arcname=os.path.basename(json_filename))\n",
    "    \n",
    "    # remove the temporary JSON file\n",
    "    os.remove(json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to load the feature vector dictionary from disk\n",
    "def load_feature_vectors(filename):\n",
    "    # extract the JSON file from the tar.gz file\n",
    "    with tarfile.open(filename, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "        json_filename = tar.getnames()[0]  # assuming only one file in the archive\n",
    "    \n",
    "    # load the JSON file and convert Python lists back to numpy arrays\n",
    "    with open(json_filename, 'r') as f:\n",
    "        feature_vectors_dict = json.load(f)\n",
    "    \n",
    "    feature_vectors = {}\n",
    "    for class_name, vectors in feature_vectors_dict.items():\n",
    "        feature_vectors[class_name] = [np.array(vector) for vector in vectors]\n",
    "    \n",
    "    # remove the extracted JSON file\n",
    "    os.remove(json_filename)\n",
    "    \n",
    "    return feature_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/validation/test subsets were loaded from a pre-generated file\n",
      "\tNumber of train files: 4799\n",
      "\tNumber of val files: 1599\n",
      "\tNumber of test files: 1601\n"
     ]
    }
   ],
   "source": [
    "# define file directory\n",
    "directory = '../data/interim/PatternNet/PatternNet/images'\n",
    "\n",
    "# create a list of classes considered for this project\n",
    "classes = ['beach', 'chaparral', 'dense_residential', 'forest', 'freeway', 'harbor', 'overpass', 'parking_space', 'river', 'swimming_pool']\n",
    "\n",
    "# define the train, val, and test sets\n",
    "train_files, val_files, test_files = generate_splits(classes, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the set of feature vectors for all images in each class\n",
    "feature_vectors, hog_images = generate_feature_vectors(train_files, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['parking_space', 'beach', 'forest', 'overpass', 'river', 'dense_residential', 'swimming_pool', 'chaparral', 'freeway', 'harbor'])\n",
      "<class 'list'>\n",
      "479\n",
      "1308\n"
     ]
    }
   ],
   "source": [
    "# inspections\n",
    "\n",
    "print(type(feature_vectors))\n",
    "print(feature_vectors.keys())\n",
    "print(type(feature_vectors['beach']))\n",
    "print(len(feature_vectors['beach']))\n",
    "print(len(feature_vectors['beach'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save feature vector to disk\n",
    "save_feature_vectors(feature_vectors, \"../data/processed/feature_vectors_1.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature vector from disk\n",
    "feature_vectors = load_feature_vectors(\"../data/processed/feature_vectors_1.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['parking_space', 'beach', 'forest', 'overpass', 'river', 'dense_residential', 'swimming_pool', 'chaparral', 'freeway', 'harbor'])\n",
      "<class 'list'>\n",
      "479\n",
      "1308\n"
     ]
    }
   ],
   "source": [
    "# inspections\n",
    "\n",
    "print(type(feature_vectors))\n",
    "print(feature_vectors.keys())\n",
    "print(type(feature_vectors['beach']))\n",
    "print(len(feature_vectors['beach']))\n",
    "print(len(feature_vectors['beach'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_281a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
