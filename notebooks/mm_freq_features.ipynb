{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate the ML subsets\n",
    "def generate_splits(classes, directory, split_file=\"../data/processed/split_definition.json\"):\n",
    "    # initialize empty lists to store train, validation, and test filenames\n",
    "    train_files, val_files, test_files = [], [], []\n",
    "    \n",
    "    # check if split definition file exists\n",
    "    if os.path.exists(split_file):\n",
    "        print(\"train/validation/test subsets were loaded from a pre-generated file\")\n",
    "        # load split definition file\n",
    "        with open(split_file, 'r') as file:\n",
    "            split_data = json.load(file)\n",
    "            train_files = split_data['Train']\n",
    "            val_files = split_data['Validation']\n",
    "            test_files = split_data['Test']\n",
    "            \n",
    "    else:\n",
    "        print(\"train/validation/test subsets were generated and saved to a file\")\n",
    "        # iterate over each class\n",
    "        for class_name in classes:\n",
    "            # get the directory path for the current class\n",
    "            class_dir = os.path.join(directory, class_name)\n",
    "            # list all files in the directory\n",
    "            files = os.listdir(class_dir)\n",
    "            # shuffle the list of files\n",
    "            np.random.shuffle(files)\n",
    "            # calculate split points\n",
    "            total_files = len(files)\n",
    "            train_split = int(total_files * 0.6)\n",
    "            val_split = int(total_files * 0.2)\n",
    "            # assign files to train, validation, and test sets\n",
    "            train_files.extend([(class_name, file) for file in files[:train_split]])\n",
    "            val_files.extend([(class_name, file) for file in files[train_split:train_split+val_split]])\n",
    "            test_files.extend([(class_name, file) for file in files[train_split+val_split:]])\n",
    "            \n",
    "        # shuffle the train, validation, and test sets\n",
    "        np.random.shuffle(train_files)\n",
    "        np.random.shuffle(val_files)\n",
    "        np.random.shuffle(test_files)\n",
    "        \n",
    "        # save split definition to a json file\n",
    "        with open(split_file, 'w') as file:\n",
    "            json.dump({'Train': train_files, 'Validation': val_files, 'Test': test_files}, file)\n",
    "            \n",
    "    # display the number of files in each set\n",
    "    print(\"\\tNumber of train files:\", len(train_files))\n",
    "    print(\"\\tNumber of val files:\", len(val_files))\n",
    "    print(\"\\tNumber of test files:\", len(test_files))\n",
    "    \n",
    "    return train_files, val_files, test_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file directory\n",
    "directory = '../data/interim/PatternNet/images'\n",
    "\n",
    "# create a list of all possible classes\n",
    "all_classes = []\n",
    "for item in os.listdir(directory):\n",
    "    item_path = os.path.join(directory, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        all_classes.append(item)\n",
    "#print(all_classes)\n",
    "        \n",
    "# create a list of classes considered for this project\n",
    "classes = ['beach', 'chaparral', 'dense_residential', 'forest', 'freeway', 'harbor', 'overpass', 'parking_space', 'river', 'swimming_pool']\n",
    "        \n",
    "# inspect the number of images per class\n",
    "data = []\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(directory, class_name)\n",
    "    image_count = len(os.listdir(class_dir))\n",
    "    data.append([class_name, image_count])\n",
    "image_count_df = pd.DataFrame(data, columns=['Class', 'Total Image Count'])\n",
    "print(\"The following classes were selected for evaluation:\")\n",
    "display(image_count_df)\n",
    "\n",
    "# define the train, val, and test sets\n",
    "train_files, val_files, test_files = generate_splits(classes, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_image(image):\n",
    "\n",
    "    # Read the image\n",
    "    # image = plt.imread(image)\n",
    "\n",
    "    # Convert the image to grayscale if it's not already\n",
    "    if len(image.shape) > 2:\n",
    "        image_gray = np.mean(image, axis=2)\n",
    "    else:\n",
    "        image_gray = image\n",
    "\n",
    "    # Compute the 2D FFT of the grayscale image\n",
    "    fft_image = np.fft.fft2(image_gray)\n",
    "\n",
    "    # Shift the zero frequency component to the center\n",
    "    fft_image_shifted = np.fft.fftshift(fft_image)\n",
    "\n",
    "    # Compute the magnitude spectrum (absolute value) of the shifted FFT\n",
    "    magnitude_spectrum = np.abs(fft_image_shifted)\n",
    "    \n",
    "    return magnitude_spectrum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to show a grid of spectrum images in a directory (given a file subset)\n",
    "def generate_freq_spectrum(files, directory, images_per_class=3):\n",
    "    # create a dictionary to store class images\n",
    "    class_images = {}\n",
    "    \n",
    "    # iterate over each file\n",
    "    for class_name, file_name in files:\n",
    "        # load the image\n",
    "        img = plt.imread(os.path.join(directory, class_name, file_name))\n",
    "        # if class not in dictionary, initialize empty list\n",
    "        if class_name not in class_images:\n",
    "            class_images[class_name] = []\n",
    "        # append image to class list\n",
    "        class_images[class_name].append(img)\n",
    "\n",
    "    # create a grid of images\n",
    "    num_classes = len(class_images)\n",
    "    fig, axes = plt.subplots(num_classes, images_per_class + 1, figsize=(12, 3*num_classes))\n",
    "    for i, (class_name, images) in enumerate(class_images.items()):\n",
    "        # display class name in the first column\n",
    "        axes[i, 0].text(0.5, 0.5, class_name, fontsize=16, ha='center', va='center')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # display random images in the subsequent columns\n",
    "        random.shuffle(images)\n",
    "        for j in range(images_per_class):\n",
    "            magnitude_spectrum = fft_image(images[j])\n",
    "            axes[i, j+1].imshow(np.log(1 + magnitude_spectrum), cmap='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a grid with 3 random images per class\n",
    "generate_freq_spectrum(train_files, directory, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to show a grid of spectrum images in a directory (given a file subset)\n",
    "def generate_freq_spectrum(files, directory, images_per_class=3):\n",
    "    # create a dictionary to store class images\n",
    "    class_images = {}\n",
    "    \n",
    "    # iterate over each file\n",
    "    for class_name, file_name in files:\n",
    "        # load the image\n",
    "        img = plt.imread(os.path.join(directory, class_name, file_name))\n",
    "        # if class not in dictionary, initialize empty list\n",
    "        if class_name not in class_images:\n",
    "            class_images[class_name] = []\n",
    "        # append image to class list\n",
    "        class_images[class_name].append(img)\n",
    "\n",
    "    # create a grid of images\n",
    "    num_classes = len(class_images)\n",
    "    fig, axes = plt.subplots(num_classes, images_per_class + 1, figsize=(12, 3*num_classes))\n",
    "    for i, (class_name, images) in enumerate(class_images.items()):\n",
    "        # display class name in the first column\n",
    "        axes[i, 0].text(0.5, 0.5, class_name, fontsize=16, ha='center', va='center')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # display random images in the subsequent columns\n",
    "        random.shuffle(images)\n",
    "        for j in range(images_per_class):\n",
    "            magnitude_spectrum = fft_image(images[j])\n",
    "            axes[i, j+1].imshow(np.log(1 + magnitude_spectrum), cmap='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
