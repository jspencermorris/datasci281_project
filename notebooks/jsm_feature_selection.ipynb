{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import tarfile\n",
    "import datetime\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "#from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage import data, exposure\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that determines the mean and standard deviation of each RGB\n",
    "# color-space channel for an image\n",
    "def compute_channel_stats(image_path):\n",
    "    # read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # compute mean and standard deviation for each color channel (RGB)\n",
    "    mean_rgb, std_rgb = cv2.meanStdDev(img)\n",
    "    \n",
    "    # flatten the results into a feature vector\n",
    "    channel_stats = np.concatenate((mean_rgb.flatten(), std_rgb.flatten()))\n",
    "    \n",
    "    return channel_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate a grid of smoothed distributions of mean intensity counts in each channel\n",
    "# for each class across all images in each class\n",
    "def compute_channel_distributions(image_path, bins=20, channels='hsv'):\n",
    "    \n",
    "    if channels=='rgb':\n",
    "        ch1 = 'r' #'Red'\n",
    "        ch2 =  'g' #'Green'\n",
    "        ch3 = 'b' #'Blue'\n",
    "    elif channels=='lab':\n",
    "        ch1 = 'L*'\n",
    "        ch2 = 'a*'\n",
    "        ch3 = 'b*'\n",
    "    elif channels=='hsv':\n",
    "        ch1 = 'h' #'Hue'\n",
    "        ch2 = 's' #'Saturation'\n",
    "        ch3 = 'v' #'Value'\n",
    "\n",
    "    # Load the image\n",
    "    img = plt.imread(image_path)\n",
    "    if channels=='lab':\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB) # convert image to L*a*b* color space\n",
    "    elif channels=='hsv':\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # convert image to HSV color space\n",
    "    \n",
    "    # calculate histograms for each color channel\n",
    "    ch1_hist, _ = np.histogram(img[:,:,0], bins=bins, range=(0, 255))\n",
    "    ch2_hist, _ = np.histogram(img[:,:,1], bins=bins, range=(0, 255))\n",
    "    ch3_hist, _ = np.histogram(img[:,:,2], bins=bins, range=(0, 255))\n",
    "      \n",
    "    # generate a vector that concatenates all 3 channel distributions\n",
    "    ch_distributions = np.concatenate((ch1_hist, ch2_hist, ch3_hist))\n",
    "    \n",
    "    # generate a list of feature names\n",
    "    feature_names = [f\"{ch1}_{i}\" for i in range(1,bins+1)] + [f\"{ch2}_{i}\" for i in range(1,bins+1)] + [f\"{ch3}_{i}\" for i in range(1,bins+1)]\n",
    "        \n",
    "\n",
    "    return ch_distributions, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that determines the hog descriptors for an image's grayscale representation\n",
    "def compute_hog_stats(image_path):\n",
    "    # read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # convert image to grayscale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # compute HOG features\n",
    "    fd = hog(gray_img, orientations=4, pixels_per_cell=(32, 32), feature_vector=True)\n",
    "    \n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_glcms(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  \n",
    "    \n",
    "    # distance between pixels\n",
    "    distances = [1]  \n",
    "    # angles for texture computation\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4] \n",
    "    glcm = graycomatrix(img, distances, angles, symmetric=True, normed=True)\n",
    "    \n",
    "    contrast = graycoprops(glcm, 'contrast').ravel().mean()\n",
    "    dissimilarity = graycoprops(glcm, 'dissimilarity').ravel().mean()\n",
    "    homogeneity = graycoprops(glcm, 'homogeneity').ravel().mean()\n",
    "    energy = graycoprops(glcm, 'energy').ravel().mean()\n",
    "    correlation = graycoprops(glcm, 'correlation').ravel().mean()\n",
    "    \n",
    "    return [contrast, dissimilarity, homogeneity, energy, correlation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fft(image_path, filt):\n",
    "    FREQBINS = 25\n",
    "\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  \n",
    "    filtered_image = apply_gaussian_filter(img, filt)\n",
    "    magnitude_spectrum = fft_image(filtered_image)\n",
    "    spec = np.log(1+magnitude_spectrum).ravel()\n",
    "            \n",
    "    hist, bins = np.histogram(spec, bins=FREQBINS)\n",
    "    \n",
    "    # We don't particularly care about the exact bin boundaries. \n",
    "    # Just need the distribution of freq spectrum.\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that loops through each file to generate a dictionary that contains\n",
    "# the feature vectors of all images in each class\n",
    "def generate_feature_vectors(files, directory):\n",
    "    feature_vectors = {}\n",
    "\n",
    "    # The 256x256 was based on EDA on the image size across the various classes.\n",
    "    # The window size 25 was established by experimentation on image blurring.\n",
    "    filt = create_gaussian_filter(256, 256, 25)\n",
    "    \n",
    "    # iterate over each file\n",
    "    for class_name, file_name in files:\n",
    "        # load the image\n",
    "        img_path = os.path.join(directory, class_name, file_name)\n",
    "\n",
    "        # compute color statistics\n",
    "        channel_stats = compute_channel_stats(img_path)\n",
    "        \n",
    "        # compute channel distributions\n",
    "        channel_distributions, ch_dist_names = compute_channel_distributions(img_path)\n",
    "        \n",
    "        # compute HOG features\n",
    "        hog_stats = compute_hog_stats(img_path)\n",
    "        \n",
    "        # compute GLCM texture features\n",
    "        glcm_features = compute_glcms(img_path)\n",
    "        \n",
    "        # compute Freq spectrum features\n",
    "        freq_features = compute_fft(img_path, filt)\n",
    "    \n",
    "        combined_features = np.concatenate((channel_stats, channel_distributions, hog_stats, glcm_features, freq_features))\n",
    "        \n",
    "        # append each combined_features array to the correct class in feature_vectors\n",
    "        if class_name not in feature_vectors:\n",
    "            feature_vectors[class_name] = []\n",
    "        feature_vectors[class_name].append(combined_features)\n",
    "        \n",
    "    hog_feature_names = [f\"hog_{i}\" for i in range(hog_stats.shape[0])]\n",
    "    frq_feature_names = [f\"freqbin_{i}\" for i in range(freq_features.shape[0])]\n",
    "    \n",
    "    return feature_vectors, ch_dist_names, hog_feature_names, frq_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to save the feature vector dictionary to disk\n",
    "def save_feature_data(feature_vectors, feature_names, file_directory):\n",
    "    \n",
    "    # save vectors\n",
    "    vectors_filename = os.path.join(file_directory, 'feature_vectors_2.tar.gz')\n",
    "   \n",
    "    # convert numpy arrays to Python lists\n",
    "    feature_vectors_dict = {}\n",
    "    for class_name, vectors in feature_vectors.items():\n",
    "        feature_vectors_dict[class_name] = [vector.tolist() for vector in vectors]\n",
    "    \n",
    "    # save feature_vectors_dict dictionary as JSON\n",
    "    json_filename = vectors_filename.replace('.tar.gz', '.json')\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(feature_vectors_dict, f)\n",
    "    \n",
    "    # create tar.gz file\n",
    "    with tarfile.open(vectors_filename, 'w:gz') as tar:\n",
    "        tar.add(json_filename, arcname=os.path.basename(json_filename))\n",
    "    \n",
    "    # remove the temporary JSON file\n",
    "    os.remove(json_filename)\n",
    "    \n",
    "    # save names\n",
    "    names_filename = os.path.join(file_directory, 'feature_names_2.pkl')\n",
    "    with open(names_filename, 'wb') as f:\n",
    "        pickle.dump(feature_names, f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file directory\n",
    "directory = '../data/interim/PatternNet/PatternNet/images'\n",
    "\n",
    "# create a list of classes considered for this project\n",
    "classes = ['beach', 'chaparral', 'dense_residential', 'forest', 'freeway', 'harbor', 'overpass', 'parking_space', 'river', 'swimming_pool']\n",
    "\n",
    "# define the train, val, and test sets\n",
    "train_files, val_files, test_files = generate_splits(classes, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# generate the set of feature vectors for all images in each class\n",
    "feature_vectors, ch_dist_names, hog_feature_names, frq_feature_names = generate_feature_vectors(train_files, directory)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time taken for training:\", elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspections\n",
    "print(type(feature_vectors))\n",
    "print(feature_vectors.keys())\n",
    "print(type(feature_vectors['beach']))\n",
    "print(len(feature_vectors['beach']))\n",
    "print(len(feature_vectors['beach'][0]))\n",
    "print(feature_vectors['beach'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of feature names\n",
    "rgb_names = ['r_mean','g_mean','b_mean','r_std','g_std','b_std'] # 6\n",
    "hsv_names = ch_dist_names # 60\n",
    "hog_names = hog_feature_names # 1296\n",
    "frq_names = frq_feature_names # 25\n",
    "texture_names = ['contrast_mean','dissimilarity_mean','homogeneity_mean','energy_mean','correlation_mean'] # 5\n",
    "#sift_names = ['']\n",
    "#freq_names = ['']\n",
    "feature_names = rgb_names + hsv_names + hog_names + texture_names + frq_names\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save feature data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save feature data to disk\n",
    "save_feature_data(feature_vectors, feature_names, \"../data/processed/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
