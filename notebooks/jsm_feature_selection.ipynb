{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import tarfile\n",
    "import datetime\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "#from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage import data, exposure\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Global knobs\n",
    "debug = False\n",
    "\n",
    "# Number of KMeans clusters to create for SIFT Bag of Visual Words\n",
    "NUM_KMEANS_CLUSTER = 20\n",
    "\n",
    "# Set this to zero, if all images in train/val/test sets have to be examined\n",
    "# For shorter test runs, set this to a reasonable non-zero value (like 100) for\n",
    "# quick functional verification\n",
    "LIMIT_NUM_IMAGES = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that determines the mean and standard deviation of each RGB\n",
    "# color-space channel for an image\n",
    "def compute_channel_stats(image_path):\n",
    "    # read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # compute mean and standard deviation for each color channel (RGB)\n",
    "    mean_rgb, std_rgb = cv2.meanStdDev(img)\n",
    "    \n",
    "    # flatten the results into a feature vector\n",
    "    channel_stats = np.concatenate((mean_rgb.flatten(), std_rgb.flatten()))\n",
    "    \n",
    "    return channel_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate a grid of smoothed distributions of mean intensity counts in each channel\n",
    "# for each class across all images in each class\n",
    "def compute_channel_distributions(image_path, bins=20, channels='hsv'):\n",
    "    \n",
    "    if channels=='rgb':\n",
    "        ch1 = 'r' #'Red'\n",
    "        ch2 =  'g' #'Green'\n",
    "        ch3 = 'b' #'Blue'\n",
    "    elif channels=='lab':\n",
    "        ch1 = 'L*'\n",
    "        ch2 = 'a*'\n",
    "        ch3 = 'b*'\n",
    "    elif channels=='hsv':\n",
    "        ch1 = 'h' #'Hue'\n",
    "        ch2 = 's' #'Saturation'\n",
    "        ch3 = 'v' #'Value'\n",
    "\n",
    "    # Load the image\n",
    "    img = plt.imread(image_path)\n",
    "    if channels=='lab':\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB) # convert image to L*a*b* color space\n",
    "    elif channels=='hsv':\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # convert image to HSV color space\n",
    "    \n",
    "    # calculate histograms for each color channel\n",
    "    ch1_hist, _ = np.histogram(img[:,:,0], bins=bins, range=(0, 255))\n",
    "    ch2_hist, _ = np.histogram(img[:,:,1], bins=bins, range=(0, 255))\n",
    "    ch3_hist, _ = np.histogram(img[:,:,2], bins=bins, range=(0, 255))\n",
    "      \n",
    "    # generate a vector that concatenates all 3 channel distributions\n",
    "    ch_distributions = np.concatenate((ch1_hist, ch2_hist, ch3_hist))\n",
    "    \n",
    "    # generate a list of feature names\n",
    "    feature_names = [f\"{ch1}_{i}\" for i in range(1,bins+1)] + [f\"{ch2}_{i}\" for i in range(1,bins+1)] + [f\"{ch3}_{i}\" for i in range(1,bins+1)]\n",
    "        \n",
    "\n",
    "    return ch_distributions, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that determines the hog descriptors for an image's grayscale representation\n",
    "def compute_hog_stats(image_path):\n",
    "    # read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # convert image to grayscale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # compute HOG features\n",
    "    fd = hog(gray_img, orientations=4, pixels_per_cell=(32, 32), feature_vector=True)\n",
    "    \n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_glcms(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  \n",
    "    \n",
    "    # distance between pixels\n",
    "    distances = [1]  \n",
    "    # angles for texture computation\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4] \n",
    "    glcm = graycomatrix(img, distances, angles, symmetric=True, normed=True)\n",
    "    \n",
    "    contrast = graycoprops(glcm, 'contrast').ravel().mean()\n",
    "    dissimilarity = graycoprops(glcm, 'dissimilarity').ravel().mean()\n",
    "    homogeneity = graycoprops(glcm, 'homogeneity').ravel().mean()\n",
    "    energy = graycoprops(glcm, 'energy').ravel().mean()\n",
    "    correlation = graycoprops(glcm, 'correlation').ravel().mean()\n",
    "    \n",
    "    return [contrast, dissimilarity, homogeneity, energy, correlation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fft(image_path, filt):\n",
    "    FREQBINS = 25\n",
    "\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  \n",
    "    filtered_image = apply_gaussian_filter(img, filt)\n",
    "    magnitude_spectrum = fft_image(filtered_image)\n",
    "    spec = np.log(1+magnitude_spectrum).ravel()\n",
    "            \n",
    "    hist, bins = np.histogram(spec, bins=FREQBINS)\n",
    "    \n",
    "    # We don't particularly care about the exact bin boundaries. \n",
    "    # Just need the distribution of freq spectrum.\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_points(img):\n",
    "\n",
    "    # Converting image to grayscale\n",
    "    gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Applying SIFT detector\n",
    "    sift = cv2.SIFT_create(nfeatures=0, nOctaveLayers=3, edgeThreshold=0.2, contrastThreshold=0.07)\n",
    "\n",
    "    #kp = sift.detect(gray, None)\n",
    "    kp, des = sift.detectAndCompute(gray,None)\n",
    "\n",
    "    # Marking the keypoint on the image using circles\n",
    "    sift_img=cv2.drawKeypoints(gray, kp, img,\n",
    "                          flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "    return sift_img, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract SIFT keypoint descriptors \n",
    "def generate_sift_vectors (files, directory):\n",
    "\n",
    "    # create a list to store SIFT Keypoint descriptors\n",
    "    sift_vectors = []\n",
    "    \n",
    "    test_num = 0\n",
    "    \n",
    "    # iterate over each file\n",
    "    for class_name, file_name in files:\n",
    "        \n",
    "        # load the image\n",
    "        img_path = os.path.join(directory, class_name, file_name)\n",
    "        img = plt.imread(img_path)\n",
    "\n",
    "        # Extract SIFT keypoints\n",
    "        sift_image, kp_descriptors = extract_key_points(img)\n",
    "\n",
    "        sift_vectors.append((kp_descriptors, class_name))\n",
    "        \n",
    "        test_num += 1     \n",
    "        if LIMIT_NUM_IMAGES > 0 and test_num > LIMIT_NUM_IMAGES:\n",
    "            break\n",
    "\n",
    "    return sift_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sift_BoW_hist(files, directory):\n",
    "    feature_vectors = {}\n",
    "    \n",
    "    sift_vectors = generate_sift_vectors(files, directory)\n",
    "    \n",
    "    # Stack all the SIFT kp descriptors of all images vertically \n",
    "    # so that we get one giant 2D array: \n",
    "    # Num keypoints across all images x size of each descriptor (128)\n",
    "    vector_kps, vector_class = sift_vectors[0]\n",
    "    vStack = np.array(vector_kps)\n",
    "    for remaining in sift_vectors[1:]:\n",
    "        vector_kps, vector_class = remaining\n",
    "        if vector_kps is not None:\n",
    "            vStack = np.vstack((vStack, vector_kps))\n",
    "    if debug:\n",
    "        print(vStack.shape)\n",
    "    \n",
    "    # Form clusters of visual words using KMeans\n",
    "    kmeans = KMeans(init=\"k-means++\", n_clusters=NUM_KMEANS_CLUSTER, n_init=4)\n",
    "    kmeans_fit = kmeans.fit_predict(vStack)\n",
    "\n",
    "    if debug:\n",
    "        print(f'Number of KPs: {kmeans_fit.shape}, MinCluster: {kmeans_fit.min()}, MaxCluster: {kmeans_fit.max()}')\n",
    "\n",
    "    # Create a histogram for each image - each kp for the image belongs to one \n",
    "    # bucket (visual word). Keep a count of such visual words per image.\n",
    "    num_images = len(sift_vectors)\n",
    "    histogram = np.array([np.zeros(NUM_KMEANS_CLUSTER) for i in range(num_images)])\n",
    "    count = 0\n",
    "    for img_num in range(num_images):\n",
    "        vector_kps, vector_class = sift_vectors[img_num]\n",
    "        if vector_kps is None:\n",
    "            # Some images have zero keypoints. \n",
    "            num_kp_in_image = 0\n",
    "        else:\n",
    "            num_kp_in_image = len(vector_kps)\n",
    "        for j in range(num_kp_in_image):\n",
    "            idx = kmeans_fit[count+j]\n",
    "            histogram[img_num][idx] += 1\n",
    "        count += num_kp_in_image\n",
    "\n",
    "        # append each combined_features array to the correct class in feature_vectors\n",
    "        if vector_class not in feature_vectors:\n",
    "            feature_vectors[vector_class] = []\n",
    "        feature_vectors[vector_class].append(histogram[img_num])\n",
    "        \n",
    "    sift_BoW_names = [f\"sift_bow_{i}\" for i in range(NUM_KMEANS_CLUSTER)]\n",
    "    \n",
    "    return feature_vectors, sift_BoW_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that loops through each file to generate a dictionary that contains\n",
    "# the feature vectors of all images in each class\n",
    "def generate_feature_vectors(files, directory):\n",
    "    feature_vectors = {}\n",
    "    image_list = {}\n",
    "    test_num = 0\n",
    "\n",
    "    # The 256x256 was based on EDA on the image size across the various classes.\n",
    "    # The window size 25 was established by experimentation on image blurring.\n",
    "    filt = create_gaussian_filter(256, 256, 25)\n",
    "    \n",
    "    # iterate over each file\n",
    "    for class_name, file_name in files:\n",
    "        # load the image\n",
    "        img_path = os.path.join(directory, class_name, file_name)\n",
    "\n",
    "        # compute color statistics\n",
    "        channel_stats = compute_channel_stats(img_path)\n",
    "        \n",
    "        # compute channel distributions\n",
    "        channel_distributions, ch_dist_names = compute_channel_distributions(img_path)\n",
    "        \n",
    "        # compute HOG features\n",
    "        hog_stats = compute_hog_stats(img_path)\n",
    "        \n",
    "        # compute GLCM texture features\n",
    "        glcm_features = compute_glcms(img_path)\n",
    "        \n",
    "        # compute Freq spectrum features\n",
    "        freq_features = compute_fft(img_path, filt)\n",
    "    \n",
    "        combined_features = np.concatenate((channel_stats, channel_distributions, hog_stats, glcm_features, freq_features))\n",
    "        \n",
    "        # append each combined_features array to the correct class in feature_vectors\n",
    "        if class_name not in feature_vectors:\n",
    "            feature_vectors[class_name] = []\n",
    "        feature_vectors[class_name].append(combined_features)\n",
    "        \n",
    "        if class_name not in image_list:\n",
    "            image_list[class_name] = []\n",
    "        image_list[class_name].append(img_path)\n",
    "        \n",
    "        test_num += 1\n",
    "        if LIMIT_NUM_IMAGES > 0 and test_num >= LIMIT_NUM_IMAGES:\n",
    "            break\n",
    "        \n",
    "    hog_feature_names = [f\"hog_{i}\" for i in range(hog_stats.shape[0])]\n",
    "    frq_feature_names = [f\"freqbin_{i}\" for i in range(freq_features.shape[0])]\n",
    "    \n",
    "    return feature_vectors, image_list, ch_dist_names, hog_feature_names, frq_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to save the feature vector dictionary to disk\n",
    "def save_feature_data(feature_vectors, images_in_class, feature_names, file_directory, file_desc):\n",
    "    \n",
    "    # save vectors\n",
    "    vectors_filename = os.path.join(file_directory, 'feature_vectors_' + file_desc + '.tar.gz')\n",
    "   \n",
    "    # convert numpy arrays to Python lists\n",
    "    feature_vectors_dict = {}\n",
    "    for class_name, vectors in feature_vectors.items():\n",
    "        feature_vectors_dict[class_name] = [vector.tolist() for vector in vectors]\n",
    "\n",
    "    # save feature_vectors_dict dictionary as JSON\n",
    "    json_filename = vectors_filename.replace('.tar.gz', '.json')\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(feature_vectors_dict, f)\n",
    "    \n",
    "    if debug:\n",
    "        display_counts_per_class(json_filename, feature_vectors_dict)\n",
    "    \n",
    "    # create tar.gz file\n",
    "    with tarfile.open(vectors_filename, 'w:gz') as tar:\n",
    "        tar.add(json_filename, arcname=os.path.basename(json_filename))\n",
    "    \n",
    "    # remove the temporary JSON file\n",
    "    os.remove(json_filename)\n",
    "\n",
    "    # save image list\n",
    "    images_filename = os.path.join(file_directory, 'image_list_' + file_desc + '.tar.gz')\n",
    "    \n",
    "    # save feature_vectors_dict dictionary as JSON\n",
    "    json_filename = images_filename.replace('.tar.gz', '.json')\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(images_in_class, f)\n",
    "\n",
    "    if debug:\n",
    "        display_counts_per_class(json_filename, images_in_class)\n",
    "\n",
    "    # create tar.gz file\n",
    "    with tarfile.open(images_filename, 'w:gz') as tar:\n",
    "        tar.add(json_filename, arcname=os.path.basename(json_filename))\n",
    "    \n",
    "    # remove the temporary JSON file\n",
    "    os.remove(json_filename)\n",
    "    \n",
    "    # save names\n",
    "    names_filename = os.path.join(file_directory, 'feature_names.pkl')\n",
    "    with open(names_filename, 'wb') as f:\n",
    "        pickle.dump(feature_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/validation/test subsets were loaded from a pre-generated file\n",
      "\tNumber of train files: 4799\n",
      "\tNumber of val files: 1599\n",
      "\tNumber of test files: 1601\n"
     ]
    }
   ],
   "source": [
    "# define file directory\n",
    "directory = '../data/interim/PatternNet/PatternNet/images'\n",
    "\n",
    "# create a list of classes considered for this project\n",
    "classes = ['beach', 'chaparral', 'dense_residential', 'forest', 'freeway', 'harbor', 'overpass', 'parking_space', 'river', 'swimming_pool']\n",
    "\n",
    "# define the train, val, and test sets\n",
    "train_files, val_files, test_files = generate_splits(classes, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_consolidated_feature_vectors(files, directory):\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # generate the set of feature vectors for all images in each class of the training set\n",
    "    feature_vectors, image_list, ch_dist_names, hog_feature_names, frq_feature_names = generate_feature_vectors(files, directory)\n",
    "\n",
    "    # generate BoVW histogram using SIFT features;\n",
    "    # Add them to the feature_vectors_train, so that the rest of the \n",
    "    # processign is common\n",
    "    sift_feature_vectors, sift_feature_names = generate_sift_BoW_hist(files, directory)\n",
    "\n",
    "    # Concatenate feature_vectors_train and sift_feature_vectors\n",
    "    for cls in feature_vectors.keys():\n",
    "        for idx,val in enumerate(feature_vectors[cls]):\n",
    "            feature_vectors[cls][idx] = np.hstack((feature_vectors[cls][idx], \\\n",
    "                                    sift_feature_vectors[cls][idx]))\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Time taken for training:\", elapsed_time)\n",
    "\n",
    "    return feature_vectors, image_list, ch_dist_names, hog_feature_names, frq_feature_names, sift_feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training: 0:28:06.761285\n"
     ]
    }
   ],
   "source": [
    "# Invoke wrapper function to combine all features including SIFT\n",
    "\n",
    "feature_vectors_train, image_list_train, ch_dist_names, hog_feature_names, frq_feature_names, sift_feature_names  = generate_consolidated_feature_vectors(train_files, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['parking_space', 'beach', 'forest', 'overpass', 'river', 'dense_residential', 'swimming_pool', 'chaparral', 'freeway', 'harbor'])\n",
      "(1412,)\n",
      "479\n",
      "1412\n",
      "[113.95576477 127.46353149 119.84751892 ...  22.           1.\n",
      "   5.        ]\n"
     ]
    }
   ],
   "source": [
    "# inspections\n",
    "\n",
    "print(type(feature_vectors_train))\n",
    "print(feature_vectors_train.keys())\n",
    "print(feature_vectors_train['beach'][0].shape)\n",
    "print(len(feature_vectors_train['beach']))\n",
    "print(len(feature_vectors_train['beach'][0]))\n",
    "print(feature_vectors_train['beach'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r_mean', 'g_mean', 'b_mean', 'r_std', 'g_std', 'b_std', 'h_1', 'h_2', 'h_3', 'h_4', 'h_5', 'h_6', 'h_7', 'h_8', 'h_9', 'h_10', 'h_11', 'h_12', 'h_13', 'h_14', 'h_15', 'h_16', 'h_17', 'h_18', 'h_19', 'h_20', 's_1', 's_2', 's_3', 's_4', 's_5', 's_6', 's_7', 's_8', 's_9', 's_10', 's_11', 's_12', 's_13', 's_14', 's_15', 's_16', 's_17', 's_18', 's_19', 's_20', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14', 'v_15', 'v_16', 'v_17', 'v_18', 'v_19', 'v_20', 'hog_0', 'hog_1', 'hog_2', 'hog_3', 'hog_4', 'hog_5', 'hog_6', 'hog_7', 'hog_8', 'hog_9', 'hog_10', 'hog_11', 'hog_12', 'hog_13', 'hog_14', 'hog_15', 'hog_16', 'hog_17', 'hog_18', 'hog_19', 'hog_20', 'hog_21', 'hog_22', 'hog_23', 'hog_24', 'hog_25', 'hog_26', 'hog_27', 'hog_28', 'hog_29', 'hog_30', 'hog_31', 'hog_32', 'hog_33', 'hog_34', 'hog_35', 'hog_36', 'hog_37', 'hog_38', 'hog_39', 'hog_40', 'hog_41', 'hog_42', 'hog_43', 'hog_44', 'hog_45', 'hog_46', 'hog_47', 'hog_48', 'hog_49', 'hog_50', 'hog_51', 'hog_52', 'hog_53', 'hog_54', 'hog_55', 'hog_56', 'hog_57', 'hog_58', 'hog_59', 'hog_60', 'hog_61', 'hog_62', 'hog_63', 'hog_64', 'hog_65', 'hog_66', 'hog_67', 'hog_68', 'hog_69', 'hog_70', 'hog_71', 'hog_72', 'hog_73', 'hog_74', 'hog_75', 'hog_76', 'hog_77', 'hog_78', 'hog_79', 'hog_80', 'hog_81', 'hog_82', 'hog_83', 'hog_84', 'hog_85', 'hog_86', 'hog_87', 'hog_88', 'hog_89', 'hog_90', 'hog_91', 'hog_92', 'hog_93', 'hog_94', 'hog_95', 'hog_96', 'hog_97', 'hog_98', 'hog_99', 'hog_100', 'hog_101', 'hog_102', 'hog_103', 'hog_104', 'hog_105', 'hog_106', 'hog_107', 'hog_108', 'hog_109', 'hog_110', 'hog_111', 'hog_112', 'hog_113', 'hog_114', 'hog_115', 'hog_116', 'hog_117', 'hog_118', 'hog_119', 'hog_120', 'hog_121', 'hog_122', 'hog_123', 'hog_124', 'hog_125', 'hog_126', 'hog_127', 'hog_128', 'hog_129', 'hog_130', 'hog_131', 'hog_132', 'hog_133', 'hog_134', 'hog_135', 'hog_136', 'hog_137', 'hog_138', 'hog_139', 'hog_140', 'hog_141', 'hog_142', 'hog_143', 'hog_144', 'hog_145', 'hog_146', 'hog_147', 'hog_148', 'hog_149', 'hog_150', 'hog_151', 'hog_152', 'hog_153', 'hog_154', 'hog_155', 'hog_156', 'hog_157', 'hog_158', 'hog_159', 'hog_160', 'hog_161', 'hog_162', 'hog_163', 'hog_164', 'hog_165', 'hog_166', 'hog_167', 'hog_168', 'hog_169', 'hog_170', 'hog_171', 'hog_172', 'hog_173', 'hog_174', 'hog_175', 'hog_176', 'hog_177', 'hog_178', 'hog_179', 'hog_180', 'hog_181', 'hog_182', 'hog_183', 'hog_184', 'hog_185', 'hog_186', 'hog_187', 'hog_188', 'hog_189', 'hog_190', 'hog_191', 'hog_192', 'hog_193', 'hog_194', 'hog_195', 'hog_196', 'hog_197', 'hog_198', 'hog_199', 'hog_200', 'hog_201', 'hog_202', 'hog_203', 'hog_204', 'hog_205', 'hog_206', 'hog_207', 'hog_208', 'hog_209', 'hog_210', 'hog_211', 'hog_212', 'hog_213', 'hog_214', 'hog_215', 'hog_216', 'hog_217', 'hog_218', 'hog_219', 'hog_220', 'hog_221', 'hog_222', 'hog_223', 'hog_224', 'hog_225', 'hog_226', 'hog_227', 'hog_228', 'hog_229', 'hog_230', 'hog_231', 'hog_232', 'hog_233', 'hog_234', 'hog_235', 'hog_236', 'hog_237', 'hog_238', 'hog_239', 'hog_240', 'hog_241', 'hog_242', 'hog_243', 'hog_244', 'hog_245', 'hog_246', 'hog_247', 'hog_248', 'hog_249', 'hog_250', 'hog_251', 'hog_252', 'hog_253', 'hog_254', 'hog_255', 'hog_256', 'hog_257', 'hog_258', 'hog_259', 'hog_260', 'hog_261', 'hog_262', 'hog_263', 'hog_264', 'hog_265', 'hog_266', 'hog_267', 'hog_268', 'hog_269', 'hog_270', 'hog_271', 'hog_272', 'hog_273', 'hog_274', 'hog_275', 'hog_276', 'hog_277', 'hog_278', 'hog_279', 'hog_280', 'hog_281', 'hog_282', 'hog_283', 'hog_284', 'hog_285', 'hog_286', 'hog_287', 'hog_288', 'hog_289', 'hog_290', 'hog_291', 'hog_292', 'hog_293', 'hog_294', 'hog_295', 'hog_296', 'hog_297', 'hog_298', 'hog_299', 'hog_300', 'hog_301', 'hog_302', 'hog_303', 'hog_304', 'hog_305', 'hog_306', 'hog_307', 'hog_308', 'hog_309', 'hog_310', 'hog_311', 'hog_312', 'hog_313', 'hog_314', 'hog_315', 'hog_316', 'hog_317', 'hog_318', 'hog_319', 'hog_320', 'hog_321', 'hog_322', 'hog_323', 'hog_324', 'hog_325', 'hog_326', 'hog_327', 'hog_328', 'hog_329', 'hog_330', 'hog_331', 'hog_332', 'hog_333', 'hog_334', 'hog_335', 'hog_336', 'hog_337', 'hog_338', 'hog_339', 'hog_340', 'hog_341', 'hog_342', 'hog_343', 'hog_344', 'hog_345', 'hog_346', 'hog_347', 'hog_348', 'hog_349', 'hog_350', 'hog_351', 'hog_352', 'hog_353', 'hog_354', 'hog_355', 'hog_356', 'hog_357', 'hog_358', 'hog_359', 'hog_360', 'hog_361', 'hog_362', 'hog_363', 'hog_364', 'hog_365', 'hog_366', 'hog_367', 'hog_368', 'hog_369', 'hog_370', 'hog_371', 'hog_372', 'hog_373', 'hog_374', 'hog_375', 'hog_376', 'hog_377', 'hog_378', 'hog_379', 'hog_380', 'hog_381', 'hog_382', 'hog_383', 'hog_384', 'hog_385', 'hog_386', 'hog_387', 'hog_388', 'hog_389', 'hog_390', 'hog_391', 'hog_392', 'hog_393', 'hog_394', 'hog_395', 'hog_396', 'hog_397', 'hog_398', 'hog_399', 'hog_400', 'hog_401', 'hog_402', 'hog_403', 'hog_404', 'hog_405', 'hog_406', 'hog_407', 'hog_408', 'hog_409', 'hog_410', 'hog_411', 'hog_412', 'hog_413', 'hog_414', 'hog_415', 'hog_416', 'hog_417', 'hog_418', 'hog_419', 'hog_420', 'hog_421', 'hog_422', 'hog_423', 'hog_424', 'hog_425', 'hog_426', 'hog_427', 'hog_428', 'hog_429', 'hog_430', 'hog_431', 'hog_432', 'hog_433', 'hog_434', 'hog_435', 'hog_436', 'hog_437', 'hog_438', 'hog_439', 'hog_440', 'hog_441', 'hog_442', 'hog_443', 'hog_444', 'hog_445', 'hog_446', 'hog_447', 'hog_448', 'hog_449', 'hog_450', 'hog_451', 'hog_452', 'hog_453', 'hog_454', 'hog_455', 'hog_456', 'hog_457', 'hog_458', 'hog_459', 'hog_460', 'hog_461', 'hog_462', 'hog_463', 'hog_464', 'hog_465', 'hog_466', 'hog_467', 'hog_468', 'hog_469', 'hog_470', 'hog_471', 'hog_472', 'hog_473', 'hog_474', 'hog_475', 'hog_476', 'hog_477', 'hog_478', 'hog_479', 'hog_480', 'hog_481', 'hog_482', 'hog_483', 'hog_484', 'hog_485', 'hog_486', 'hog_487', 'hog_488', 'hog_489', 'hog_490', 'hog_491', 'hog_492', 'hog_493', 'hog_494', 'hog_495', 'hog_496', 'hog_497', 'hog_498', 'hog_499', 'hog_500', 'hog_501', 'hog_502', 'hog_503', 'hog_504', 'hog_505', 'hog_506', 'hog_507', 'hog_508', 'hog_509', 'hog_510', 'hog_511', 'hog_512', 'hog_513', 'hog_514', 'hog_515', 'hog_516', 'hog_517', 'hog_518', 'hog_519', 'hog_520', 'hog_521', 'hog_522', 'hog_523', 'hog_524', 'hog_525', 'hog_526', 'hog_527', 'hog_528', 'hog_529', 'hog_530', 'hog_531', 'hog_532', 'hog_533', 'hog_534', 'hog_535', 'hog_536', 'hog_537', 'hog_538', 'hog_539', 'hog_540', 'hog_541', 'hog_542', 'hog_543', 'hog_544', 'hog_545', 'hog_546', 'hog_547', 'hog_548', 'hog_549', 'hog_550', 'hog_551', 'hog_552', 'hog_553', 'hog_554', 'hog_555', 'hog_556', 'hog_557', 'hog_558', 'hog_559', 'hog_560', 'hog_561', 'hog_562', 'hog_563', 'hog_564', 'hog_565', 'hog_566', 'hog_567', 'hog_568', 'hog_569', 'hog_570', 'hog_571', 'hog_572', 'hog_573', 'hog_574', 'hog_575', 'hog_576', 'hog_577', 'hog_578', 'hog_579', 'hog_580', 'hog_581', 'hog_582', 'hog_583', 'hog_584', 'hog_585', 'hog_586', 'hog_587', 'hog_588', 'hog_589', 'hog_590', 'hog_591', 'hog_592', 'hog_593', 'hog_594', 'hog_595', 'hog_596', 'hog_597', 'hog_598', 'hog_599', 'hog_600', 'hog_601', 'hog_602', 'hog_603', 'hog_604', 'hog_605', 'hog_606', 'hog_607', 'hog_608', 'hog_609', 'hog_610', 'hog_611', 'hog_612', 'hog_613', 'hog_614', 'hog_615', 'hog_616', 'hog_617', 'hog_618', 'hog_619', 'hog_620', 'hog_621', 'hog_622', 'hog_623', 'hog_624', 'hog_625', 'hog_626', 'hog_627', 'hog_628', 'hog_629', 'hog_630', 'hog_631', 'hog_632', 'hog_633', 'hog_634', 'hog_635', 'hog_636', 'hog_637', 'hog_638', 'hog_639', 'hog_640', 'hog_641', 'hog_642', 'hog_643', 'hog_644', 'hog_645', 'hog_646', 'hog_647', 'hog_648', 'hog_649', 'hog_650', 'hog_651', 'hog_652', 'hog_653', 'hog_654', 'hog_655', 'hog_656', 'hog_657', 'hog_658', 'hog_659', 'hog_660', 'hog_661', 'hog_662', 'hog_663', 'hog_664', 'hog_665', 'hog_666', 'hog_667', 'hog_668', 'hog_669', 'hog_670', 'hog_671', 'hog_672', 'hog_673', 'hog_674', 'hog_675', 'hog_676', 'hog_677', 'hog_678', 'hog_679', 'hog_680', 'hog_681', 'hog_682', 'hog_683', 'hog_684', 'hog_685', 'hog_686', 'hog_687', 'hog_688', 'hog_689', 'hog_690', 'hog_691', 'hog_692', 'hog_693', 'hog_694', 'hog_695', 'hog_696', 'hog_697', 'hog_698', 'hog_699', 'hog_700', 'hog_701', 'hog_702', 'hog_703', 'hog_704', 'hog_705', 'hog_706', 'hog_707', 'hog_708', 'hog_709', 'hog_710', 'hog_711', 'hog_712', 'hog_713', 'hog_714', 'hog_715', 'hog_716', 'hog_717', 'hog_718', 'hog_719', 'hog_720', 'hog_721', 'hog_722', 'hog_723', 'hog_724', 'hog_725', 'hog_726', 'hog_727', 'hog_728', 'hog_729', 'hog_730', 'hog_731', 'hog_732', 'hog_733', 'hog_734', 'hog_735', 'hog_736', 'hog_737', 'hog_738', 'hog_739', 'hog_740', 'hog_741', 'hog_742', 'hog_743', 'hog_744', 'hog_745', 'hog_746', 'hog_747', 'hog_748', 'hog_749', 'hog_750', 'hog_751', 'hog_752', 'hog_753', 'hog_754', 'hog_755', 'hog_756', 'hog_757', 'hog_758', 'hog_759', 'hog_760', 'hog_761', 'hog_762', 'hog_763', 'hog_764', 'hog_765', 'hog_766', 'hog_767', 'hog_768', 'hog_769', 'hog_770', 'hog_771', 'hog_772', 'hog_773', 'hog_774', 'hog_775', 'hog_776', 'hog_777', 'hog_778', 'hog_779', 'hog_780', 'hog_781', 'hog_782', 'hog_783', 'hog_784', 'hog_785', 'hog_786', 'hog_787', 'hog_788', 'hog_789', 'hog_790', 'hog_791', 'hog_792', 'hog_793', 'hog_794', 'hog_795', 'hog_796', 'hog_797', 'hog_798', 'hog_799', 'hog_800', 'hog_801', 'hog_802', 'hog_803', 'hog_804', 'hog_805', 'hog_806', 'hog_807', 'hog_808', 'hog_809', 'hog_810', 'hog_811', 'hog_812', 'hog_813', 'hog_814', 'hog_815', 'hog_816', 'hog_817', 'hog_818', 'hog_819', 'hog_820', 'hog_821', 'hog_822', 'hog_823', 'hog_824', 'hog_825', 'hog_826', 'hog_827', 'hog_828', 'hog_829', 'hog_830', 'hog_831', 'hog_832', 'hog_833', 'hog_834', 'hog_835', 'hog_836', 'hog_837', 'hog_838', 'hog_839', 'hog_840', 'hog_841', 'hog_842', 'hog_843', 'hog_844', 'hog_845', 'hog_846', 'hog_847', 'hog_848', 'hog_849', 'hog_850', 'hog_851', 'hog_852', 'hog_853', 'hog_854', 'hog_855', 'hog_856', 'hog_857', 'hog_858', 'hog_859', 'hog_860', 'hog_861', 'hog_862', 'hog_863', 'hog_864', 'hog_865', 'hog_866', 'hog_867', 'hog_868', 'hog_869', 'hog_870', 'hog_871', 'hog_872', 'hog_873', 'hog_874', 'hog_875', 'hog_876', 'hog_877', 'hog_878', 'hog_879', 'hog_880', 'hog_881', 'hog_882', 'hog_883', 'hog_884', 'hog_885', 'hog_886', 'hog_887', 'hog_888', 'hog_889', 'hog_890', 'hog_891', 'hog_892', 'hog_893', 'hog_894', 'hog_895', 'hog_896', 'hog_897', 'hog_898', 'hog_899', 'hog_900', 'hog_901', 'hog_902', 'hog_903', 'hog_904', 'hog_905', 'hog_906', 'hog_907', 'hog_908', 'hog_909', 'hog_910', 'hog_911', 'hog_912', 'hog_913', 'hog_914', 'hog_915', 'hog_916', 'hog_917', 'hog_918', 'hog_919', 'hog_920', 'hog_921', 'hog_922', 'hog_923', 'hog_924', 'hog_925', 'hog_926', 'hog_927', 'hog_928', 'hog_929', 'hog_930', 'hog_931', 'hog_932', 'hog_933', 'hog_934', 'hog_935', 'hog_936', 'hog_937', 'hog_938', 'hog_939', 'hog_940', 'hog_941', 'hog_942', 'hog_943', 'hog_944', 'hog_945', 'hog_946', 'hog_947', 'hog_948', 'hog_949', 'hog_950', 'hog_951', 'hog_952', 'hog_953', 'hog_954', 'hog_955', 'hog_956', 'hog_957', 'hog_958', 'hog_959', 'hog_960', 'hog_961', 'hog_962', 'hog_963', 'hog_964', 'hog_965', 'hog_966', 'hog_967', 'hog_968', 'hog_969', 'hog_970', 'hog_971', 'hog_972', 'hog_973', 'hog_974', 'hog_975', 'hog_976', 'hog_977', 'hog_978', 'hog_979', 'hog_980', 'hog_981', 'hog_982', 'hog_983', 'hog_984', 'hog_985', 'hog_986', 'hog_987', 'hog_988', 'hog_989', 'hog_990', 'hog_991', 'hog_992', 'hog_993', 'hog_994', 'hog_995', 'hog_996', 'hog_997', 'hog_998', 'hog_999', 'hog_1000', 'hog_1001', 'hog_1002', 'hog_1003', 'hog_1004', 'hog_1005', 'hog_1006', 'hog_1007', 'hog_1008', 'hog_1009', 'hog_1010', 'hog_1011', 'hog_1012', 'hog_1013', 'hog_1014', 'hog_1015', 'hog_1016', 'hog_1017', 'hog_1018', 'hog_1019', 'hog_1020', 'hog_1021', 'hog_1022', 'hog_1023', 'hog_1024', 'hog_1025', 'hog_1026', 'hog_1027', 'hog_1028', 'hog_1029', 'hog_1030', 'hog_1031', 'hog_1032', 'hog_1033', 'hog_1034', 'hog_1035', 'hog_1036', 'hog_1037', 'hog_1038', 'hog_1039', 'hog_1040', 'hog_1041', 'hog_1042', 'hog_1043', 'hog_1044', 'hog_1045', 'hog_1046', 'hog_1047', 'hog_1048', 'hog_1049', 'hog_1050', 'hog_1051', 'hog_1052', 'hog_1053', 'hog_1054', 'hog_1055', 'hog_1056', 'hog_1057', 'hog_1058', 'hog_1059', 'hog_1060', 'hog_1061', 'hog_1062', 'hog_1063', 'hog_1064', 'hog_1065', 'hog_1066', 'hog_1067', 'hog_1068', 'hog_1069', 'hog_1070', 'hog_1071', 'hog_1072', 'hog_1073', 'hog_1074', 'hog_1075', 'hog_1076', 'hog_1077', 'hog_1078', 'hog_1079', 'hog_1080', 'hog_1081', 'hog_1082', 'hog_1083', 'hog_1084', 'hog_1085', 'hog_1086', 'hog_1087', 'hog_1088', 'hog_1089', 'hog_1090', 'hog_1091', 'hog_1092', 'hog_1093', 'hog_1094', 'hog_1095', 'hog_1096', 'hog_1097', 'hog_1098', 'hog_1099', 'hog_1100', 'hog_1101', 'hog_1102', 'hog_1103', 'hog_1104', 'hog_1105', 'hog_1106', 'hog_1107', 'hog_1108', 'hog_1109', 'hog_1110', 'hog_1111', 'hog_1112', 'hog_1113', 'hog_1114', 'hog_1115', 'hog_1116', 'hog_1117', 'hog_1118', 'hog_1119', 'hog_1120', 'hog_1121', 'hog_1122', 'hog_1123', 'hog_1124', 'hog_1125', 'hog_1126', 'hog_1127', 'hog_1128', 'hog_1129', 'hog_1130', 'hog_1131', 'hog_1132', 'hog_1133', 'hog_1134', 'hog_1135', 'hog_1136', 'hog_1137', 'hog_1138', 'hog_1139', 'hog_1140', 'hog_1141', 'hog_1142', 'hog_1143', 'hog_1144', 'hog_1145', 'hog_1146', 'hog_1147', 'hog_1148', 'hog_1149', 'hog_1150', 'hog_1151', 'hog_1152', 'hog_1153', 'hog_1154', 'hog_1155', 'hog_1156', 'hog_1157', 'hog_1158', 'hog_1159', 'hog_1160', 'hog_1161', 'hog_1162', 'hog_1163', 'hog_1164', 'hog_1165', 'hog_1166', 'hog_1167', 'hog_1168', 'hog_1169', 'hog_1170', 'hog_1171', 'hog_1172', 'hog_1173', 'hog_1174', 'hog_1175', 'hog_1176', 'hog_1177', 'hog_1178', 'hog_1179', 'hog_1180', 'hog_1181', 'hog_1182', 'hog_1183', 'hog_1184', 'hog_1185', 'hog_1186', 'hog_1187', 'hog_1188', 'hog_1189', 'hog_1190', 'hog_1191', 'hog_1192', 'hog_1193', 'hog_1194', 'hog_1195', 'hog_1196', 'hog_1197', 'hog_1198', 'hog_1199', 'hog_1200', 'hog_1201', 'hog_1202', 'hog_1203', 'hog_1204', 'hog_1205', 'hog_1206', 'hog_1207', 'hog_1208', 'hog_1209', 'hog_1210', 'hog_1211', 'hog_1212', 'hog_1213', 'hog_1214', 'hog_1215', 'hog_1216', 'hog_1217', 'hog_1218', 'hog_1219', 'hog_1220', 'hog_1221', 'hog_1222', 'hog_1223', 'hog_1224', 'hog_1225', 'hog_1226', 'hog_1227', 'hog_1228', 'hog_1229', 'hog_1230', 'hog_1231', 'hog_1232', 'hog_1233', 'hog_1234', 'hog_1235', 'hog_1236', 'hog_1237', 'hog_1238', 'hog_1239', 'hog_1240', 'hog_1241', 'hog_1242', 'hog_1243', 'hog_1244', 'hog_1245', 'hog_1246', 'hog_1247', 'hog_1248', 'hog_1249', 'hog_1250', 'hog_1251', 'hog_1252', 'hog_1253', 'hog_1254', 'hog_1255', 'hog_1256', 'hog_1257', 'hog_1258', 'hog_1259', 'hog_1260', 'hog_1261', 'hog_1262', 'hog_1263', 'hog_1264', 'hog_1265', 'hog_1266', 'hog_1267', 'hog_1268', 'hog_1269', 'hog_1270', 'hog_1271', 'hog_1272', 'hog_1273', 'hog_1274', 'hog_1275', 'hog_1276', 'hog_1277', 'hog_1278', 'hog_1279', 'hog_1280', 'hog_1281', 'hog_1282', 'hog_1283', 'hog_1284', 'hog_1285', 'hog_1286', 'hog_1287', 'hog_1288', 'hog_1289', 'hog_1290', 'hog_1291', 'hog_1292', 'hog_1293', 'hog_1294', 'hog_1295', 'contrast_mean', 'dissimilarity_mean', 'homogeneity_mean', 'energy_mean', 'correlation_mean', 'freqbin_0', 'freqbin_1', 'freqbin_2', 'freqbin_3', 'freqbin_4', 'freqbin_5', 'freqbin_6', 'freqbin_7', 'freqbin_8', 'freqbin_9', 'freqbin_10', 'freqbin_11', 'freqbin_12', 'freqbin_13', 'freqbin_14', 'freqbin_15', 'freqbin_16', 'freqbin_17', 'freqbin_18', 'freqbin_19', 'freqbin_20', 'freqbin_21', 'freqbin_22', 'freqbin_23', 'freqbin_24', 'sift_bow_0', 'sift_bow_1', 'sift_bow_2', 'sift_bow_3', 'sift_bow_4', 'sift_bow_5', 'sift_bow_6', 'sift_bow_7', 'sift_bow_8', 'sift_bow_9', 'sift_bow_10', 'sift_bow_11', 'sift_bow_12', 'sift_bow_13', 'sift_bow_14', 'sift_bow_15', 'sift_bow_16', 'sift_bow_17', 'sift_bow_18', 'sift_bow_19']\n"
     ]
    }
   ],
   "source": [
    "# create a list of feature names\n",
    "rgb_names = ['r_mean','g_mean','b_mean','r_std','g_std','b_std'] # 6\n",
    "hsv_names = ch_dist_names # 60\n",
    "hog_names = hog_feature_names # 1296\n",
    "texture_names = ['contrast_mean','dissimilarity_mean','homogeneity_mean','energy_mean','correlation_mean'] # 5\n",
    "frq_names = frq_feature_names # 25\n",
    "sift_names = sift_feature_names\n",
    "feature_names = rgb_names + hsv_names + hog_names + texture_names + frq_names + sift_names\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for training: 0:04:23.960015\n",
      "Time taken for training: 0:04:35.339901\n"
     ]
    }
   ],
   "source": [
    "# generate the set of feature vectors for all images in each class of the training set\n",
    "feature_vectors_val, image_list_val, ch_dist_names, hog_feature_names, frq_feature_names, sift_feature_names = generate_consolidated_feature_vectors(val_files, directory)\n",
    "\n",
    "# generate the set of feature vectors for all images in each class of the training set\n",
    "feature_vectors_test, image_list_test, ch_dist_names, hog_feature_names, frq_feature_names, sift_feature_names = generate_consolidated_feature_vectors(test_files, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save feature data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save feature data to disk\n",
    "save_feature_data(feature_vectors_train, image_list_train, feature_names, \"../data/processed/\", \"train\")\n",
    "save_feature_data(feature_vectors_val, image_list_val, feature_names, \"../data/processed/\", \"val\")\n",
    "save_feature_data(feature_vectors_test, image_list_test, feature_names, \"../data/processed/\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_vectors_val['river'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_list_val['river'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
